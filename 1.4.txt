1.
(i)Volume – The name 'Big Data' itself is related to a size which is enormous. 
Size of data plays very crucial role in determining value out of data. 
Also, whether a particular data can actually be considered as a Big Data or not, is dependent upon volume of data. 
Hence, 'Volume' is one characteristic which needs to be considered while dealing with 'Big Data'.

(ii)Variety – The next aspect of 'Big Data' is its variety.
Variety refers to heterogeneous sources and the nature of data, both structured and unstructured. 
During earlier days, spreadsheets and databases were the only sources of data considered by most of the applications. 
Now days, data in the form of emails, photos, videos, monitoring devices, PDFs, audio, etc. is also being considered in the analysis applications.
 This variety of unstructured data poses certain issues for storage, mining and analysing data.

(iii)Velocity – The term 'velocity' refers to the speed of generation of data. 
How fast the data is generated and processed to meet the demands, determines real potential in the data.
Big Data Velocity deals with the speed at which data flows in from sources like business processes, application logs, networks 
and social media sites, sensors, Mobile devices, etc. The flow of data is massive and continuous.

(iv)Variability – This refers to the inconsistency which can be shown by the data at times,
 thus hampering the process of being able to handle and manage the data effectively.

2.solutions to big data
i)scaling out:Scaling out means linking together other lower-performance machines to collectively do the work of a much more advanced one. 
	With these types of distributed setups, it's easy to handle a larger workload by running data through different system trajectories.
	There are a variety of benefits and disadvantages to each approach. 
	Scaling up can be expensive, and ultimately, some experts argue that it's not viable because of the limits to individual hardware pieces on the market. 
	However, it does make it easier to control a system, and to provide for certain data quality issues.
 ii)scaling up:Scaling up generally refers to purchasing and installing a more capable central control or piece of hardware. 
	For example, when a project’s input/output demands start to push against the limits of an individual server,
 	a scaling up approach would be to buy a more capable server with more processing capacity and RAM.
3. 
 They are fundamentally different ways of addressing the need for more processor capacity, memory and other resources.
Scaling up generally refers to purchasing and installing a more capable central control or piece of hardware. 
	For example, when a project’s input/output demands start to push against the limits of an individual server,
 	a scaling up approach would be to buy a more capable server with more processing capacity and RAM.
Scaling out means linking together other lower-performance machines to collectively do the work of a much more advanced one. 
	With these types of distributed setups, it's easy to handle a larger workload by running data through different system trajectories.
	There are a variety of benefits and disadvantages to each approach. 
	Scaling up can be expensive, and ultimately, some experts argue that it's not viable because of the limits to individual hardware pieces on the market. 
	However, it does make it easier to control a system, and to provide for certain data quality issues.

